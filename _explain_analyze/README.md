Структура проекта

/
├─ README.md
├─ py_pg_explain_analyzer/
│ ├─ __init__.py
│ ├─ models.py # dataclasses и pydantic‑схемы результата
│ ├─ db.py # коннектор к БД (psycopg/psycopg2/asyncpg — по выбору)
│ ├─ analyzer.py # основной класс PgExplainAnalyzer
│ ├─ sql_parser.py # это статический анализатор SQL-запросов на базе sqlglot
│ ├─ sql_metadata_extractor.py # извлекаем доп инфомрцию о полях используемых в запросе (индексы а также инфу о таблице)
│ ├─ plan_parser.py # парсинг EXPLAIN FORMAT JSON -> внутренние структуры
│ ├─ rules.py # набор эвристик и генерация индексов
│ └─ llm_summarizer.py # вызов LLM 
└─ demo.py


Суть: 
- на этапе инициализации подается: строка подключения к базе данных и клиент к LLM
- приходит SQL запрос 
- делаем первинчный разбор в sql_parser.py (на базе sqlglot)
    - определяем таблицы
    - и колонки которые важны для расчетов (все те которые не select)
    - делаем нормализацию запроса
- собираем дополнительную информацию из sql_metadata_extractor.py
    - индексы 
    - статистика по таблицам
- db.py - выполняем EXPLAIN
- прогоняем EXPLAIN через набор эвристик (rules.py) для явного выявления важных моментов
- объединяем все полученные данные: 
    - исходный SQL запрос,  
    - первинчный разбор от sql_parser   
    - дополнительная информация о колонках и таблицах  
    - результат EXPLAIN  
    - результат обарботки rules
- направляем в LLM для получения: списка проблем и предупреждений, рекомендаций и исправленного запроса.



### Модуль db.py подключаемсся к БД и можем выполнить EXPLAIN
Метод `db.PgConnection.explain_json()` выполняет `EXPLAIN (..., FORMAT JSON)` и возвращает сырой JSON.
По умолчанию analyze отключен, но можно включить на страх и риск передав соответсчтующий прамтер (рекомендуется не включать). 


### Модуль sql_parser.py — это статический анализатор SQL-запросов на базе sqlglot.
    1. Парсинг и AST
        - разбирает SQL-запрос в дерево выражений (sqlglot.Expression),
        - доступ к полному AST для дальнейшего анализа.
    2. Извлечение таблиц и колонок
        - собирает список используемых таблиц (включая schema.table),
        - учитывает алиасы,
        - исключает CTE (WITH ...),
        - пропускает * (SELECT *),
        - отдельно выделяет колонки, участвующие в фильтрах/сортировках/группировках (filter_columns) — полезно для рекомендаций по индексам.
    3. Нормализация SQL
        - возвращает отформатированный SQL в едином стиле (pretty-print).
    4. Линтинг
        - выполняет базовые проверки стиля: наличие ; в конце, количество таблиц,
        - можно расширять правила (например, запрет SELECT *, вложенные подзапросы и т. п.).


### Модуль sql_metadata_extractor.py извлекаем доп инфомрцию о полях используемых в запросе (индексы а также инфу о таблице)
Что измеряется:   
1. Типы данных колонок (columns)  
- Извлекается информация о типе каждой колонки из filter_columns.  
- Это нужно, чтобы понимать:  
- правильные ли типы используются для условий (date vs text),  
- можно ли строить индексы (например, B-Tree для чисел/строк, GIN/GiST для JSON/Text).  
2. Наличие индексов по важным колонкам (indexes)  
- Смотрим только индексы, где участвует хотя бы одна колонка из filter_columns.  
- Это помогает проверить:  
- оптимизированы ли JOIN, WHERE, GROUP BY, ORDER BY,  
- нет ли избыточных или «лишних» индексов.  
3. Оценка количества строк в таблице (estimated_rows)  
- Используется значение pg_class.reltuples.  
- Это приближённая оценка размера таблицы (кол-во строк), которая влияет на выбор плана выполнения (seq scan vs index scan).  
4. Статистика по колонкам (columns_stats)  
- Достаётся из pg_stats для каждой колонки из filter_columns:  
    - n_distinct → сколько уникальных значений (или оценка селективности),  
    - null_frac → доля NULL в колонке,  
    - most_common_vals → самые частые значения (для категориальных полей).  
- Эти данные нужны, чтобы LLM или DBA могли:  
    - оценить селективность фильтров (WHERE status='active' будет ли использовать индекс),  
    - понять, полезен ли индекс или его не будет использовать оптимизатор.   
5. Непокрытые индексами колонки (unindexed_columns)
    - колонки испоьзуемые в рассчетах, но для которых не найдено ни одного индекса.




# Режим анализа EXPLAIN без использования LLM а анализ по правилам 

## Как это работает (коротко)

1. `db.PgConnection.explain_json()` выполняет `EXPLAIN (..., FORMAT JSON)` и возвращает сырой JSON.
2. `plan_parser.parse_explain_json()` превращает ответ в `Plan/PlanNode`.
3. `rules.RULES` — набор функций‑правил, которые обходят дерево плана и генерируют `Issue/Suggestion/IndexCandidate`.
4. `analyzer.PgExplainAnalyzer` склеивает всё в `AnalysisResult` и собирает Markdown‑отчёт. Если подключён LLM — добавляет краткое резюме/приоритизацию.

---

## Идеи развития

* **Больше правил:**

  * Bitmap Heap Scan без соответствующего Bitmap Index Scan → подсказка об индексе.
  * Nested Loop с огромными `rows` → проверки на отсутствующие индексы на правой таблице.
  * Seq Scan с высокой долей `Shared Read Blocks` → индекс или партиционирование.
  * Оценка селективности по `Filter`, поиск составных индексов и `INCLUDE` колонок (covering).
  * Детектирование `ORDER BY ... LIMIT` без индекса по порядку.
  * `OFFSET` большой → ключи пагинации (keyset pagination).
* **Сбор статистики таблиц** (pg\_class/pg\_stats) для принятия более точных решений по индексам, в т.ч. многостолбцовые и extended statistics.
* **Парсинг SQL** (например, `sqlglot`) для точной привязки колонок к таблицам/алиасам.
* **Учёт схемы и существующих индексов** (просмотр `pg_indexes`) для избежания дублей.
* **Автоматические эксперименты**: создавать индекс в тестовом режиме (в транзакции), замерять повторный план.
* **Форматы отчётов**: JSON/Markdown/HTML (шаблон Jinja2).
* **Интеграция с LLM**: генерация более развёрнутых объяснений, диалоговые рекомендации.
